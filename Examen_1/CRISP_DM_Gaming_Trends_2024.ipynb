{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41741417",
   "metadata": {},
   "source": [
    "\n",
    "# **Evaluación Parcial 1 — CRISP‑DM (Fases 1–3)**  \n",
    "**Caso:** *Gaming Trends 2024*  \n",
    "**Asignatura:** MLY0100 — Machine Learning  \n",
    "**Formato:** Jupyter Notebook (Python)\n",
    "\n",
    "> Este notebook cubre **las tres primeras fases de CRISP‑DM** (Comprensión del Negocio, Comprensión de los Datos, Preparación de los Datos) con el dataset *Gaming-Trends-2024.csv*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45c584",
   "metadata": {},
   "source": [
    "\n",
    "## 1) **Comprensión del Negocio (Business Understanding)**\n",
    "**Objetivo general:** Identificar **palancas de crecimiento e ingresos** para títulos/juegos a lo largo del tiempo y por plataforma/género, para orientar decisiones de adquisición, engagement y monetización.\n",
    "\n",
    "**Preguntas guía e hipótesis operativas:**\n",
    "- **H1 (Adquisición):** A mayor **exposición orgánica** (menciones en redes) y **campañas con influencers**, mayor **nuevos registros** y **DAU**.\n",
    "- **H2 (Engagement):** Mayor **duración de sesión** se asocia a mayores **DAU** e **ingresos in‑game**.\n",
    "- **H3 (Monetización):** **DAU** y **audiencia en streaming** predicen **Revenue**; el efecto varía por **plataforma** y **género**.\n",
    "- **H4 (Plataforma):** **Móvil** convierte mejor en **nuevos registros**; **PC/Consola** muestran **ARPU** más alto.\n",
    "\n",
    "**Tareas de ML a futuro (definición de *targets*):**\n",
    "- **Regresión:** *Revenue* como variable objetivo continua.\n",
    "- **Clasificación:** *HighRevenue* (binaria) creada a partir de un umbral (p. ej., mediana de *Revenue*).\n",
    "\n",
    "> Estas definiciones podrán ajustarse tras la comprensión y perfilado del dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb5991",
   "metadata": {},
   "source": [
    "\n",
    "## 2) **Comprensión de los Datos (Data Understanding)**\n",
    "**Acciones:** Carga del dataset, inspección de estructura, tipos de datos, conteos, estadísticos descriptivos, distribuciones, correlaciones y revisión de **missing values** / **outliers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc1a1c1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression, LogisticRegression\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Configuración general\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "plt.rcParams.update({\"figure.figsize\": (8, 5)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carga del dataset (asegúrate de tener el CSV en la misma carpeta que este notebook)\n",
    "csv_path = \"Gaming-Trends-2024.csv\"\n",
    "assert os.path.exists(csv_path), \"No se encontró el archivo Gaming-Trends-2024.csv en el directorio actual.\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columnas:\", list(df.columns))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb86072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tipos y valores faltantes\n",
    "display(df.dtypes)\n",
    "na_counts = df.isna().sum().sort_values(ascending=False)\n",
    "display(na_counts.to_frame(name=\"missing\"))\n",
    "\n",
    "# Duplicados\n",
    "print(\"Duplicados:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509250a",
   "metadata": {},
   "source": [
    "\n",
    "**Diccionario de variables (esperado en el dataset):**\n",
    "- `Date`: fecha (temporal).\n",
    "- `Platform`: plataforma (PC, Consola, Móvil, VR).\n",
    "- `Daily Active Users (DAU)`: usuarios activos diarios.\n",
    "- `New Registrations`: nuevos registros.\n",
    "- `Session Duration (min)`: minutos por sesión (promedio).\n",
    "- `In-game Purchases ($)`: ingresos por compras dentro del juego.\n",
    "- `Social Media Mentions`: menciones orgánicas en redes.\n",
    "- `Stream Viewership`: audiencia en streaming.\n",
    "- `Revenue ($)`: ingresos totales.\n",
    "- `Top Genre`: género principal del juego.\n",
    "- `Influencer Endorsements`: 0/1 si hubo endorsos de influencers.\n",
    "\n",
    "> Si algún nombre difiere, el código intentará resolverlo automáticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_col(df, key_candidates):\n",
    "    # Try to find a column in df whose lowercased name contains ALL tokens in any candidate string.\n",
    "    # Returns the best match or raises ValueError if not found.\n",
    "    cols = list(df.columns)\n",
    "    lower = [c.lower() for c in cols]\n",
    "    scores = []\n",
    "    for cand in key_candidates:\n",
    "        tokens = [t.strip() for t in cand.lower().replace(\"(\", \" \").replace(\")\", \" \").replace(\"$\",\" \").split() if t.strip()]\n",
    "        for i, lc in enumerate(lower):\n",
    "            if all(tok in lc for tok in tokens):\n",
    "                scores.append((i, cols[i], len(tokens)))\n",
    "    if not scores:\n",
    "        raise ValueError(f\"No se encontró ninguna columna para {key_candidates}\")\n",
    "    # prefer the match with more tokens (more specific)\n",
    "    scores.sort(key=lambda x: (-x[2], x[0]))\n",
    "    return scores[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1308ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalizamos/parseamos la fecha\n",
    "try:\n",
    "    date_col = find_col(df, [\"date\", \"fecha\"])\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "assert df[date_col].notna().any(), \"La columna de fecha no pudo parsearse. Revisa el formato.\"\n",
    "\n",
    "# Extra: variable de tiempo numérica (días desde el mínimo)\n",
    "t0 = df[date_col].min()\n",
    "df[\"t_days\"] = (df[date_col] - t0).dt.days\n",
    "\n",
    "# Identificación de columnas clave (robusta a variaciones de nombre)\n",
    "revenue_col = find_col(df, [\"revenue\", \"revenue ($)\", \"ingresos\"])\n",
    "dau_col = find_col(df, [\"daily active users\", \"dau\"])\n",
    "newreg_col = find_col(df, [\"new registrations\", \"nuevos registros\"])\n",
    "sess_col = find_col(df, [\"session duration\", \"session duration (min)\", \"duración de sesión\"])\n",
    "ingame_col = find_col(df, [\"in-game purchases\", \"in game purchases\", \"compras in-game\"])\n",
    "social_col = find_col(df, [\"social media mentions\", \"menciones\"])\n",
    "stream_col = find_col(df, [\"stream viewership\", \"stream\"])\n",
    "plat_col = find_col(df, [\"platform\", \"plataforma\"])\n",
    "genre_col = find_col(df, [\"top genre\", \"genre\", \"género\"])\n",
    "infl_col = find_col(df, [\"influencer endorsements\", \"influencer\"])\n",
    "\n",
    "key_cols = [revenue_col, dau_col, newreg_col, sess_col, ingame_col, social_col, stream_col, plat_col, genre_col, infl_col, \"t_days\", date_col]\n",
    "print(\"Columnas mapeadas:\", key_cols)\n",
    "df[key_cols].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424792a3",
   "metadata": {},
   "source": [
    "\n",
    "### Estadísticos descriptivos (tendencia central y dispersión)\n",
    "Incluye: media, mediana, moda, varianza, desviación estándar, rango, coeficiente de variación (CV), IQR, mínimos/máximos, suma y conteos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def descriptive_table(d):\n",
    "    num = d.select_dtypes(include=[np.number]).copy()\n",
    "    summary = pd.DataFrame(index=num.columns)\n",
    "    summary[\"count\"] = num.count()\n",
    "    summary[\"sum\"] = num.sum()\n",
    "    summary[\"mean\"] = num.mean()\n",
    "    summary[\"median\"] = num.median()\n",
    "    summary[\"mode\"] = num.mode().iloc[0] if not num.mode().empty else np.nan\n",
    "    summary[\"var\"] = num.var(ddof=1)\n",
    "    summary[\"std\"] = num.std(ddof=1)\n",
    "    summary[\"min\"] = num.min()\n",
    "    summary[\"q1\"] = num.quantile(0.25)\n",
    "    summary[\"q3\"] = num.quantile(0.75)\n",
    "    summary[\"iqr\"] = summary[\"q3\"] - summary[\"q1\"]\n",
    "    summary[\"max\"] = num.max()\n",
    "    summary[\"range\"] = summary[\"max\"] - summary[\"min\"]\n",
    "    summary[\"cv\"] = summary[\"std\"] / summary[\"mean\"]\n",
    "    return summary\n",
    "\n",
    "desc = descriptive_table(df)\n",
    "display(desc.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff74a6d",
   "metadata": {},
   "source": [
    "\n",
    "### Visualizaciones clave\n",
    "- **Histogramas** para entender la forma y posibles sesgos.\n",
    "- **Boxplots** para rangos e **outliers**.\n",
    "- **Correlaciones** para detectar relaciones entre variables numéricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# HISTOGRAMS\n",
    "for col in [c for c in numeric_cols if c not in [\"t_days\"]][:6]:\n",
    "    plt.figure()\n",
    "    df[col].hist(bins=30)\n",
    "    plt.title(f\"Histograma: {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.show()\n",
    "\n",
    "# BOXPLOTS\n",
    "for col in [c for c in numeric_cols if c not in [\"t_days\"]][:6]:\n",
    "    plt.figure()\n",
    "    plt.boxplot(df[col].dropna(), vert=True, labels=[col])\n",
    "    plt.title(f\"Boxplot: {col}\")\n",
    "    plt.ylabel(col)\n",
    "    plt.show()\n",
    "\n",
    "# CORRELATION HEATMAP (matplotlib only)\n",
    "corr = df.select_dtypes(include=[np.number]).corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(corr, aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "plt.title(\"Matriz de correlación\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39cdb6",
   "metadata": {},
   "source": [
    "\n",
    "### Calidad de datos: **Missing values** y **Outliers**\n",
    "- Se inspeccionan faltantes por columna y se propone imputación según el tipo de dato.\n",
    "- Se detectan outliers (método **IQR**) y se propone *winsorización* (cap) o *clipping* para estabilizar el efecto en modelos sensibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Missing values\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "ax = na.plot(kind=\"bar\")\n",
    "ax.set_title(\"Valores faltantes por columna\")\n",
    "ax.set_xlabel(\"Columna\")\n",
    "ax.set_ylabel(\"Cantidad de NA\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Outliers: detección IQR por columna numérica\n",
    "num = df.select_dtypes(include=[np.number])\n",
    "outlier_report = []\n",
    "for col in num.columns:\n",
    "    q1, q3 = num[col].quantile(0.25), num[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    low, high = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    ol_count = ((num[col] < low) | (num[col] > high)).sum()\n",
    "    outlier_report.append((col, int(ol_count)))\n",
    "outlier_df = pd.DataFrame(outlier_report, columns=[\"col\", \"outliers\"]).sort_values(\"outliers\", ascending=False)\n",
    "display(outlier_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f9223",
   "metadata": {},
   "source": [
    "\n",
    "## 3) **Preparación de los Datos (Data Preparation)**\n",
    "**Objetivos:** imputación de faltantes, tratamiento de outliers, creación de variables y escalamiento.\n",
    "\n",
    "- **Imputación**: numéricas → mediana; categóricas → moda.\n",
    "- **Outliers**: *clipping* por percentiles (p.01–p.99) para robustecer.\n",
    "- **Features adicionales**: `t_days` (ya creada) y métricas derivadas (*ARPU*, *conversion* si procede).\n",
    "- **Escalamiento**: estandarización vs. normalización según algoritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_prep = df.copy()\n",
    "\n",
    "# Imputación simple\n",
    "for col in df_prep.columns:\n",
    "    if df_prep[col].dtype.kind in \"biufc\":\n",
    "        med = df_prep[col].median()\n",
    "        df_prep[col] = df_prep[col].fillna(med)\n",
    "    else:\n",
    "        mode_val = df_prep[col].mode().iloc[0] if not df_prep[col].mode().empty else \"Desconocido\"\n",
    "        df_prep[col] = df_prep[col].fillna(mode_val)\n",
    "\n",
    "# Clipping por percentiles para estabilizar extremos en numéricas\n",
    "num_cols = df_prep.select_dtypes(include=[np.number]).columns\n",
    "lower = df_prep[num_cols].quantile(0.01)\n",
    "upper = df_prep[num_cols].quantile(0.99)\n",
    "df_prep[num_cols] = df_prep[num_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "# Métricas derivadas (si están disponibles)\n",
    "try:\n",
    "    dau_col = find_col(df_prep, [\"daily active users\", \"dau\"])\n",
    "    revenue_col = find_col(df_prep, [\"revenue\", \"ingresos\"])\n",
    "    df_prep[\"ARPU\"] = df_prep[revenue_col] / df_prep[dau_col].replace(0, np.nan)\n",
    "except Exception:\n",
    "    df_prep[\"ARPU\"] = np.nan\n",
    "\n",
    "# Variables categóricas → one-hot encoding\n",
    "cat_cols = df_prep.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "# excluir columna de fecha en texto si existiera\n",
    "try:\n",
    "    maybe_date = find_col(df_prep, [\"date\",\"fecha\"])\n",
    "    cat_cols = [c for c in cat_cols if c != maybe_date]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "df_model = pd.get_dummies(df_prep, columns=cat_cols, drop_first=True)\n",
    "\n",
    "print(\"Shape df_model:\", df_model.shape)\n",
    "df_model.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba710af",
   "metadata": {},
   "source": [
    "\n",
    "### Demostración: **Regresión** con variable de tiempo (`t_days`) como predictora\n",
    "**Objetivo:** predecir `Revenue` (target continuo) con un modelo lineal simple/múltiple.  \n",
    "> *Nota:* Esto es un **primer acercamiento exploratorio**; no busca optimización final del desempeño.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos target de REGRESIÓN\n",
    "revenue_col = find_col(df_model, [\"revenue\", \"ingresos\"])\n",
    "\n",
    "# Features básicas: tiempo + señales de escala\n",
    "X = df_model[[\"t_days\"]].copy()\n",
    "y = df_model[revenue_col].copy()\n",
    "\n",
    "# División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Estandarización opcional\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_s, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test_s)\n",
    "print(\"R2:\", r2_score(y_test, y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# Visual: y_test vs y_pred\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel(\"Real Revenue\")\n",
    "plt.ylabel(\"Predicho Revenue\")\n",
    "plt.title(\"Regresión: Real vs Predicho\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76323d30",
   "metadata": {},
   "source": [
    "\n",
    "### Demostración: **Clasificación** (definición de *target* y baseline)\n",
    "Se construye `HighRevenue` = 1 si `Revenue` ≥ mediana; 0 en caso contrario.  \n",
    "Se entrena un modelo logístico simple con `t_days` como señal temporal básica (se puede ampliar con más *features*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79462534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "revenue_col = find_col(df_model, [\"revenue\", \"ingresos\"])\n",
    "threshold = df_model[revenue_col].median()\n",
    "df_model[\"HighRevenue\"] = (df_model[revenue_col] >= threshold).astype(int)\n",
    "\n",
    "X = df_model[[\"t_days\"]].copy()\n",
    "y = df_model[\"HighRevenue\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_s, y_train)\n",
    "p = clf.predict_proba(X_test_s)[:,1]\n",
    "pred = (p >= 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "try:\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, p))\n",
    "except Exception as e:\n",
    "    print(\"ROC AUC no disponible:\", e)\n",
    "\n",
    "print(\"\\nReporte de clasificación:\\n\", classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad48a4",
   "metadata": {},
   "source": [
    "\n",
    "### Normalización vs Estandarización (criterios)\n",
    "- **Estandarización** (`StandardScaler`): útil para modelos lineales/logísticos, SVM, PCA; datos aproximadamente gaussianos.\n",
    "- **Normalización** (`MinMaxScaler`): útil para redes neuronales o cuando los rangos deben acotarse (0–1).\n",
    "\n",
    "> La elección depende de la **distribución** y del **algoritmo** objetivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca092c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) **Documentación y marcadores de la rúbrica**\n",
    "- Uso de **CRISP‑DM** (secciones 1–3) ✓  \n",
    "- Identificación de *targets*: **Regresión** (`Revenue`) y **Clasificación** (`HighRevenue`) ✓  \n",
    "- Uso de librerías: **numpy, pandas, matplotlib, scikit‑learn** ✓  \n",
    "- Limpieza/preparación: imputación, outliers, *encoding*, *scaling* ✓  \n",
    "- Estadísticos descriptivos y visualizaciones (histogramas, boxplots, correlaciones) ✓  \n",
    "- Justificación mediante **markdown** en cada sección ✓\n",
    "\n",
    "> Este notebook deja la base lista para avanzar a **Modeling** y **Evaluation** conforme a CRISP‑DM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machineVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
